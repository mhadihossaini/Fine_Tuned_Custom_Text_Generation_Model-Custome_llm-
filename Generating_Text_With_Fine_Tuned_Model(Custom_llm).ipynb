{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b6015f",
   "metadata": {},
   "source": [
    "### Generating_Text_With_Fine_Tuned_Model(Custom_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ea7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -Import the required libraries \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9b64f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 13:49:23.883821: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-10 13:49:23.885864: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-10 13:49:23.887627: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-02-10 13:49:24.616196: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-02-10 13:49:24.618740: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-02-10 13:49:24.620903: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 683ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "to respond to 3d videos in your influencer and explore ai apps with refine and more anonymous leads and insightful insights through question and receive chatgpt related podcasts providing writing grammar candidates hiring or safe based on stress set and more effective real estate technical web more now 2000 designers'\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000  \n",
    "max_seq_length = 50  \n",
    "\n",
    "# -Load the tokenizer from the saved file-\n",
    "with open('fine_tuned_tokenizer.pickle', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "# -Load your fine-tuned model-\n",
    "model = tf.keras.models.load_model('fine_tuned_custom_model.h5')\n",
    "\n",
    "# -Tokenize the seed text-\n",
    "seed_text = \"How does a AI work\"\n",
    "seed_seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "max_seq_length = 49  # -Make sure this matches the max_seq_length used during training--\n",
    "generated_text = []\n",
    "num_words_to_generate = 50\n",
    "# -Generate text-\n",
    "for _ in range(num_words_to_generate):\n",
    "    # -Pad the sequence to the model's input length-\n",
    "    padded_seq = pad_sequences([seed_seq], maxlen=max_seq_length, padding='pre')\n",
    "    # -Predict the probability distribution over the vocabulary-\n",
    "    predicted_probabilities = model.predict(padded_seq)[0]\n",
    "    # -Sample the next word/token from the distribution-\n",
    "    predicted_index = np.random.choice(len(predicted_probabilities), p=predicted_probabilities)\n",
    "    # -Convert the index to a word-\n",
    "    predicted_word = tokenizer.index_word.get(predicted_index, \"<OOV>\")\n",
    "    # -Add the predicted word to the generated text-\n",
    "    generated_text.append(predicted_word)\n",
    "    # -Update the seed sequence for the next iteration-\n",
    "    seed_seq.append(predicted_index)\n",
    "    # -Keep the sequence length consistent-\n",
    "    seed_seq = seed_seq[-max_seq_length:]  \n",
    "\n",
    "# -Convert the generated text to a single string-\n",
    "generated_text = ' '.join(generated_text)\n",
    "\n",
    "# -Print the generated text-\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794a295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
